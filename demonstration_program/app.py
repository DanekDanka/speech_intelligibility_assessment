"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –ø–æ–∫–∞–∑–∞ —Ä–∞–±–æ—Ç—ã CNN –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è STOI.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Gradio –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.
"""
import os
import sys
import torch
import torch.nn as nn
import numpy as np
import librosa
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # –ò—Å–ø–æ–ª—å–∑—É–µ–º non-interactive backend
from pathlib import Path
import gradio as gr

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥–µ–ª–∏
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))
from model import CNNSTOIPredictor

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
SAMPLE_RATE = 16000
CHUNK_DURATION = 5.0  # 5 —Å–µ–∫—É–Ω–¥
CHUNK_STEP = 1.0  # –®–∞–≥ –≤ 1 —Å–µ–∫—É–Ω–¥—É
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
DEFAULT_CHECKPOINT = '/home/danya/develop/speech_intelligibility_assessment/checkpoints_all_models/best_cnn.pt'
CHECKPOINT_PATH = os.getenv('MODEL_CHECKPOINT', DEFAULT_CHECKPOINT)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
_loaded_model = None


def load_model(checkpoint_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞"""
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"–ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {checkpoint_path}")
    
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    if 'model_kwargs' in checkpoint:
        model_kwargs = checkpoint['model_kwargs']
    elif 'hyperparameters' in checkpoint and 'model_kwargs' in checkpoint['hyperparameters']:
        model_kwargs = checkpoint['hyperparameters']['model_kwargs']
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –ª—É—á—à–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        model_kwargs = {
            'input_dim': 1,
            'num_filters': [96, 192, 384, 768, 1536],
            'kernel_sizes': [11, 9, 7, 5, 3],
            'stride': 3,
            'dropout': 0.06,
            'use_metadata_features': False,
            'fc_hidden_dim': 320,
            'num_fc_layers': 3
        }
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = CNNSTOIPredictor(**model_kwargs).to(DEVICE)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ. –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters()):,}")
    
    return model


def split_audio_into_chunks(audio, sample_rate, chunk_duration=5.0, step=1.0):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç –∞—É–¥–∏–æ –Ω–∞ —á–∞–Ω–∫–∏ –∑–∞–¥–∞–Ω–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –∑–∞–¥–∞–Ω–Ω—ã–º —à–∞–≥–æ–º.
    
    Args:
        audio: numpy array —Å –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–º–∏
        sample_rate: —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
        chunk_duration: –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        step: —à–∞–≥ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    Returns:
        chunks: —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ (–∫–∞–∂–¥—ã–π –∫–∞–∫ numpy array)
        chunk_times: —Å–ø–∏—Å–æ–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –Ω–∞—á–∞–ª–∞ –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
    """
    chunk_samples = int(chunk_duration * sample_rate)
    step_samples = int(step * sample_rate)
    
    chunks = []
    chunk_times = []
    
    start_idx = 0
    while start_idx + chunk_samples <= len(audio):
        chunk = audio[start_idx:start_idx + chunk_samples]
        chunks.append(chunk)
        chunk_times.append(start_idx / sample_rate)
        start_idx += step_samples
    
    return chunks, chunk_times


def predict_stoi(model, audio_chunk, sample_rate=SAMPLE_RATE):
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç STOI –¥–ª—è –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ –∞—É–¥–∏–æ.
    
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        audio_chunk: numpy array —Å –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–º–∏
        sample_rate: —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
    
    Returns:
        stoi_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ STOI
    """
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∞—É–¥–∏–æ
    if len(audio_chunk) == 0:
        return 0.0
    
    # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –¥–æ –Ω—É–∂–Ω–æ–π —á–∞—Å—Ç–æ—Ç—ã, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
    if sample_rate != SAMPLE_RATE:
        audio_chunk = librosa.resample(audio_chunk, orig_sr=sample_rate, target_sr=SAMPLE_RATE)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ tensor
    audio_tensor = torch.FloatTensor(audio_chunk).to(DEVICE)
    
    # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
    audio_tensor = audio_tensor.unsqueeze(0)  # (1, seq_len)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    with torch.no_grad():
        stoi_pred = model(audio_tensor)
        stoi_pred = stoi_pred.cpu().item()
    
    return float(stoi_pred)


def process_audio(audio_file):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    
    Args:
        audio_file: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É –∏–ª–∏ tuple (sample_rate, audio_data)
    
    Returns:
        tuple: (–≥—Ä–∞—Ñ–∏–∫ STOI, —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –º–µ–ª-—Å–ø–µ–∫—Ç—Ä —Å –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏)
    """
    if audio_file is None:
        return None, "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª", None
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
        if isinstance(audio_file, tuple):
            # Gradio –ø–µ—Ä–µ–¥–∞–µ—Ç (sample_rate, audio_data)
            sample_rate, audio_data = audio_file
            if audio_data.ndim > 1:
                # –ï—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∫–∞–Ω–∞–ª
                audio = audio_data[:, 0].astype(np.float32)
            else:
                audio = audio_data.astype(np.float32)
        else:
            # –≠—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            audio, sample_rate = librosa.load(audio_file, sr=None, mono=True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∞—É–¥–∏–æ
        duration = len(audio) / sample_rate
        if duration < CHUNK_DURATION:
            return None, f"–ê—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ ({duration:.2f} —Å–µ–∫). –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º {CHUNK_DURATION} —Å–µ–∫—É–Ω–¥.", None
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio))
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks, chunk_times = split_audio_into_chunks(
            audio, sample_rate, CHUNK_DURATION, CHUNK_STEP
        )
        
        if len(chunks) == 0:
            return None, "–ê—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 5 —Å–µ–∫—É–Ω–¥)", None
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å (–¥–µ–ª–∞–µ–º —ç—Ç–æ –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ)
        global _loaded_model
        if _loaded_model is None:
            _loaded_model = load_model(CHECKPOINT_PATH)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º STOI –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
        stoi_predictions = []
        for chunk in chunks:
            stoi = predict_stoi(_loaded_model, chunk, sample_rate)
            stoi_predictions.append(stoi)
        
        stoi_predictions = np.array(stoi_predictions)
        chunk_times = np.array(chunk_times)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        mean_stoi = np.mean(stoi_predictions)
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ STOI
        fig_stoi, ax_stoi = plt.subplots(figsize=(10, 6))
        ax_stoi.plot(chunk_times, stoi_predictions, 'b-o', linewidth=2, markersize=8)
        ax_stoi.axhline(y=mean_stoi, color='r', linestyle='--', linewidth=2, label=f'–°—Ä–µ–¥–Ω–µ–µ: {mean_stoi:.3f}')
        ax_stoi.set_xlabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)', fontsize=12)
        ax_stoi.set_ylabel('STOI', fontsize=12)
        ax_stoi.set_title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è STOI –ø–æ –≤—Ä–µ–º–µ–Ω–∏', fontsize=14, fontweight='bold')
        ax_stoi.grid(True, alpha=0.3)
        ax_stoi.legend(fontsize=11)
        ax_stoi.set_ylim([0, 1])
        plt.tight_layout()
        
        # –°–æ–∑–¥–∞–µ–º –º–µ–ª-—Å–ø–µ–∫—Ç—Ä —Å –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ STOI
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å –∞—É–¥–∏–æ —Å–∏–≥–Ω–∞–ª –¥–ª—è –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–∞
        mel_spec = librosa.feature.melspectrogram(
            y=audio, sr=sample_rate, n_mels=128, hop_length=512
        )
        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        fig_mel, ax_mel = plt.subplots(figsize=(12, 6))
        
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –º–µ–ª-—Å–ø–µ–∫—Ç—Ä
        times_mel = librosa.frames_to_time(np.arange(mel_spec_db.shape[1]), sr=sample_rate, hop_length=512)
        im = ax_mel.imshow(mel_spec_db, aspect='auto', origin='lower', 
                          extent=[times_mel[0], times_mel[-1], 0, 128],
                          cmap='viridis', interpolation='bilinear')
        
        # –ù–∞–∫–ª–∞–¥—ã–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è STOI
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ —Ä–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ —Å —Ü–≤–µ—Ç–æ–º, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º –∑–Ω–∞—á–µ–Ω–∏—é STOI
        for i, (chunk_time, stoi_val) in enumerate(zip(chunk_times, stoi_predictions)):
            chunk_end = chunk_time + CHUNK_DURATION
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–≤–µ—Ç–æ–≤—É—é –∫–∞—Ä—Ç—É –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è STOI (–∑–µ–ª–µ–Ω—ã–π = –≤—ã—Å–æ–∫–∏–π, –∫—Ä–∞—Å–Ω—ã–π = –Ω–∏–∑–∫–∏–π)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º STOI –¥–ª—è —Ü–≤–µ—Ç–æ–≤–æ–π –∫–∞—Ä—Ç—ã (STOI –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1)
            color = plt.cm.RdYlGn(stoi_val)  # Red-Yellow-Green colormap
            # –†–∏—Å—É–µ–º –ø–æ–ª—É–ø—Ä–æ–∑—Ä–∞—á–Ω—ã–π –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
            rect = plt.Rectangle((chunk_time, 0), CHUNK_DURATION, 128, 
                               facecolor=color, alpha=0.4, edgecolor='white', linewidth=1.5)
            ax_mel.add_patch(rect)
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Å –∑–Ω–∞—á–µ–Ω–∏–µ–º STOI (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —á–∞–Ω–∫ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π)
            if CHUNK_DURATION >= 2.0:
                text_color = 'white' if stoi_val < 0.5 else 'black'
                ax_mel.text(chunk_time + CHUNK_DURATION / 2, 64, f'{stoi_val:.2f}',
                           ha='center', va='center', fontsize=9, fontweight='bold',
                           color=text_color, bbox=dict(boxstyle='round,pad=0.3', 
                           facecolor='white', alpha=0.7, edgecolor='black', linewidth=0.5))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ü–≤–µ—Ç–æ–≤—É—é —à–∫–∞–ª—É –¥–ª—è STOI
        from matplotlib.colors import LinearSegmentedColormap
        sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, norm=plt.Normalize(vmin=0, vmax=1))
        sm.set_array([])
        cbar2 = plt.colorbar(sm, ax=ax_mel, label='STOI', location='right', pad=0.02)
        cbar2.set_label('STOI (–Ω–∞–ª–æ–∂–µ–Ω–æ –Ω–∞ —Å–ø–µ–∫—Ç—Ä)', rotation=270, labelpad=20)
        
        ax_mel.set_xlabel('–í—Ä–µ–º—è (—Å–µ–∫—É–Ω–¥—ã)', fontsize=12)
        ax_mel.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (–º–µ–ª-–±–∏–Ω)', fontsize=12)
        ax_mel.set_title('–ú–µ–ª-—Å–ø–µ–∫—Ç—Ä —Å –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ STOI', fontsize=14, fontweight='bold')
        plt.colorbar(im, ax=ax_mel, label='–ê–º–ø–ª–∏—Ç—É–¥–∞ (dB)')
        plt.tight_layout()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        result_text = f"""
**–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:**

- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤:** {len(chunks)}
- **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞:** {CHUNK_DURATION} —Å–µ–∫—É–Ω–¥
- **–®–∞–≥ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏:** {CHUNK_STEP} —Å–µ–∫—É–Ω–¥
- **–°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ STOI:** {mean_stoi:.4f}
- **–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ STOI:** {np.min(stoi_predictions):.4f}
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ STOI:** {np.max(stoi_predictions):.4f}
- **–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ:** {np.std(stoi_predictions):.4f}
        """
        
        return fig_stoi, result_text, fig_mel
        
    except Exception as e:
        import traceback
        error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ: {str(e)}\n\n{traceback.format_exc()}"
        return None, error_msg, None


# –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio
def create_interface():
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Gradio"""
    
    with gr.Blocks(title="STOI Prediction Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # üé§ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è STOI üé§
        
        –≠—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Ä–∞–∑–±–æ—Ä—á–∏–≤–æ—Å—Ç—å —Ä–µ—á–∏ (STOI - Speech Transmission Objective Intelligibility)
        —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–µ–Ω–Ω–æ–π CNN –º–æ–¥–µ–ª–∏.
        
        **–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:**
        1. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É "–ó–∞–ø–∏—Å–∞—Ç—å –∞—É–¥–∏–æ" –∏ –ø—Ä–æ–∏–∑–Ω–µ—Å–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ —Ä–µ—á–∏
        2. –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∞—É–¥–∏–æ —Ñ–∞–π–ª (WAV, MP3 –∏ —Ç.–¥.)
        3. –ê—É–¥–∏–æ –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ 5-—Å–µ–∫—É–Ω–¥–Ω—ã–µ –æ—Ç—Ä–µ–∑–∫–∏ —Å —à–∞–≥–æ–º 1 —Å–µ–∫—É–Ω–¥–∞
        4. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç—Ä–µ–∑–∫–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∂–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ STOI
        5. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–æ–±—Ä–∞–∑—è—Ç—Å—è –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–∞—Ö –Ω–∏–∂–µ
        """)
        
        with gr.Row():
            with gr.Column():
                audio_input = gr.Audio(
                    label="–ó–∞–ø–∏—Å–∞—Ç—å –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞—É–¥–∏–æ",
                    type="numpy",
                    sources=["microphone", "upload"]
                )
                process_btn = gr.Button("–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∞—É–¥–∏–æ", variant="primary", size="lg")
            
            with gr.Column():
                results_text = gr.Markdown(label="–†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        
        with gr.Row():
            stoi_plot = gr.Plot(label="–ì—Ä–∞—Ñ–∏–∫ –∑–Ω–∞—á–µ–Ω–∏–π STOI")
            mel_spectrogram = gr.Plot(label="–ú–µ–ª-—Å–ø–µ–∫—Ç—Ä —Å –Ω–∞–ª–æ–∂–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ STOI")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ –∫–Ω–æ–ø–∫–∏ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∞—É–¥–∏–æ
        process_btn.click(
            fn=process_audio,
            inputs=audio_input,
            outputs=[stoi_plot, results_text, mel_spectrogram]
        )
        
        audio_input.change(
            fn=process_audio,
            inputs=audio_input,
            outputs=[stoi_plot, results_text, mel_spectrogram]
        )
        
        # gr.Markdown("""
        # ---
        # **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –≤—Å–µ–≥–æ —Å —á–∏—Å—Ç–æ–π —Ä–µ—á—å—é –±–µ–∑ —Å–∏–ª—å–Ω—ã—Ö —à—É–º–æ–≤.
        # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –≤ —Ç–∏—Ö–æ–π –æ–±—Å—Ç–∞–Ω–æ–≤–∫–µ —Å —Ö–æ—Ä–æ—à–∏–º –º–∏–∫—Ä–æ—Ñ–æ–Ω–æ–º.
        # """)
    
    return demo


if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {CHECKPOINT_PATH}")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è MODEL_CHECKPOINT –∏–ª–∏ –ø–æ–º–µ—Å—Ç–∏—Ç–µ –º–æ–¥–µ–ª—å –≤ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å.")
        print("–ü—Ä–∏–º–µ—Ä: export MODEL_CHECKPOINT=../checkpoints_cnn_final/best_cnn_final.pt")
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",  # –î–æ—Å—Ç—É–ø–Ω–æ –∏–∑–≤–Ω–µ
        server_port=7860,
        share=False  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ True –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—É–±–ª–∏—á–Ω–æ–π —Å—Å—ã–ª–∫–∏
    )
